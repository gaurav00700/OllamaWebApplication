name: llm_app  # Project name
services:
  # Frontend container
  frontend:
    image: gaurav00700/llm_app-frontend:latest
    # build:
    #   context: .  # Path relative to docker-compose
    #   dockerfile: ./frontend/Dockerfile  # Dockerfile path
    container_name: llm_app-frontend
    ports:
      - 8500:8500
    restart: unless-stopped # Options: always, unless-stopped, on-failure, never
    volumes:
      - ./data:/workdir/data
    environment:
      - BACKEND_ENDPNT=http://backend:8000/ask
    networks:
      - llm_net

  # Redis container (for caching purposes)
  redis:
    image: redis:latest
    container_name: llm_app-redis
    # ports:
    #   - 6379:6379  
    restart: unless-stopped # Options: always, unless-stopped, on-failure, never
    volumes:
      - ./data:/data 
      - redis-vol:/data 
    networks:
      - llm_net

  # Backend container
  backend:
    image: gaurav00700/llm_app-backend:latest
    # build:
    #   context: .  # Path relative to docker-compose
    #   dockerfile: ./backend/Dockerfile  # Dockerfile path
    container_name: llm_app-backend
    # ports:
    #   - 8000:8000
    restart: unless-stopped # Options: always, unless-stopped, on-failure, never
    volumes:
      - ./data:/workdir/data
    environment:
      - LLM_NAME=${LLM_NAME}
      - LLM_HOST_NAME=llm
      - LLM_PORT=11434
      - REDIS_HOST_NAME=redis
      - REDIS_PORT=6379
    networks:
      - llm_net

  # LLM container
  llm:
    image: gaurav00700/llm_app-llm:latest # ollama/ollama:latest
    # build:
    #   context: .  # Path relative to docker-compose
    #   dockerfile: ./model/Dockerfile  # Dockerfile path
    container_name: llm_app-llm
    runtime: nvidia
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: ["gpu"]
              count: all
    volumes:
      # - /usr/share/ollama/.ollama:/root/.ollama   # use existing model
      - llm-vol:/root/.ollama # named volume for caching models
    # ports:
    #   - 11433:11434
    restart: unless-stopped # Options: always, unless-stopped, on-failure, never
    environment:
     - LLM_NAME=${LLM_NAME}
    networks:
      - llm_net
    # entrypoint: ["/usr/bin/bash", "/ollama_entrypoint.sh"]

networks:
  llm_net:
    driver: bridge

volumes:
  llm-vol:
    driver: local
  redis-vol:
   driver: local
